
python3 PreProcessDataFiles.py --inputData="./input-data/english-subtitles-cleaned/english-subtitles-cleaned-0[0-2][0-9].txt" --outputFileName="english-subtitles-cleaned-000-029-12k" --isTokenized --forceLowerCase --newOutputFile --outputBatchSizeMb=150 --vocabulary="conversational-english-12k-lc-12000" --tokenizer=sentencepiece --newOutputFile ;
python3 PreProcessDataFiles.py --inputData="./input-data/english-subtitles-cleaned/english-subtitles-cleaned-0[3-6][0-9].txt" --outputFileName="english-subtitles-cleaned-030-069-12k" --isTokenized --forceLowerCase --newOutputFile --outputBatchSizeMb=150 --vocabulary="conversational-english-12k-lc-12000" --tokenizer=sentencepiece --newOutputFile ;
python3 PreProcessDataFiles.py --inputData="./input-data/english-subtitles-cleaned/english-subtitles-cleaned-0[6-9][9-9].txt" --outputFileName="english-subtitles-cleaned-069-099-12k" --isTokenized --forceLowerCase --newOutputFile --outputBatchSizeMb=150 --vocabulary="conversational-english-12k-lc-12000" --tokenizer=sentencepiece --newOutputFile ;
python3 PreProcessDataFiles.py --inputData="./input-data/english-subtitles-cleaned/english-subtitles-cleaned-1[0-9][0-9].txt" --outputFileName="english-subtitles-cleaned-100-130-12k" --isTokenized --forceLowerCase --newOutputFile --outputBatchSizeMb=150 --vocabulary="conversational-english-12k-lc-12000" --tokenizer=sentencepiece --newOutputFile



------ short conv

--data
python3 OllamaGenerateTestData.py --model="llama3.2:3b" --printInterval=10 --prompt="generate a romantic, cozy or flirty conversation between 300 and 1000 words with sentences that are no larger than 40 words, only output the conversations and no other text, put the participants names between brackets [$name], do not use names in the conversation" --outputFileName="synthetic-short-romantic-conversations" --numberOfGenerations=100000

python3 OllamaGenerateTestData.py --model="llama3.2:3b" --printInterval=10 --prompt="generate a casual conversation between 300 and 1000 words with sentences that are no larger than 40 words, only output the conversations and no other text, put the participants names between brackets [$name], do not use names in the conversation" --outputFileName="synthetic-short-casual-conversations" --numberOfGenerations=100000

--tokenizer
python3 PreProcessTokenizer.py --inputData="./input-data/synthetic-**/**.txt" --vocabSize=8000 --vocabularyName="conversational-english-8k-lc" --forceLowerCase --newTrainingFile ;  
python3 PreProcessTokenizer.py --inputData="./input-data/synthetic-**/**.txt" --vocabSize=12000 --vocabularyName="conversational-english-12k-lc" --forceLowerCase --newTrainingFile ;  
python3 PreProcessTokenizer.py --inputData="./input-data/synthetic-**/**.txt" --vocabSize=16000 --vocabularyName="conversational-english-16k-lc" --forceLowerCase --newTrainingFile  

--preprocess PreProcessDataFiles
python3 PreProcessDataFiles.py --inputData="./input-data/synthetic-casual-conversations/**.txt" --outputFileName="synthetic-casual-conversations-8k" --isTokenized --forceLowerCase --newOutputFile --outputBatchSizeMb=150 --vocabulary="conversational-english-8k-lc-8000" --tokenizer=sentencepiece --newOutputFile ;
python3 PreProcessDataFiles.py --inputData="./input-data/synthetic-short-**/**.txt" --outputFileName="synthetic-short-conversations-8k" --isTokenized --forceLowerCase --newOutputFile --outputBatchSizeMb=150 --vocabulary="conversational-english-8k-lc-8000" --tokenizer=sentencepiece --newOutputFile ;
python3 PreProcessDataFiles.py --inputData="./input-data/synthetic-**/**.txt" --outputFileName="synthetic-conversations-8k" --isTokenized --forceLowerCase --newOutputFile --outputBatchSizeMb=500 --vocabulary="conversational-english-8k-lc-8000" --tokenizer=sentencepiece --newOutputFile ;

;

python3 PreProcessDataFiles.py --inputData="./input-data/synthetic-casual-conversations/**.txt" --outputFileName="synthetic-casual-conversations-12k" --isTokenized --forceLowerCase --newOutputFile --outputBatchSizeMb=150 --vocabulary="conversational-english-12k-lc-12000" --tokenizer=sentencepiece --newOutputFile ;
python3 PreProcessDataFiles.py --inputData="./input-data/synthetic-short-**/**.txt" --outputFileName="synthetic-short-conversations-12k" --isTokenized --forceLowerCase --newOutputFile --outputBatchSizeMb=150 --vocabulary="conversational-english-12k-lc-12000" --tokenizer=sentencepiece --newOutputFile ;
python3 PreProcessDataFiles.py --inputData="./input-data/synthetic-**/**.txt" --outputFileName="synthetic-conversations-12k" --isTokenized --forceLowerCase --newOutputFile --outputBatchSizeMb=500 --vocabulary="conversational-english-12k-lc-12000" --tokenizer=sentencepiece --newOutputFile ;

;

python3 PreProcessDataFiles.py --inputData="./input-data/synthetic-casual-conversations/**.txt" --outputFileName="synthetic-casual-conversations-16k" --isTokenized --forceLowerCase --newOutputFile --outputBatchSizeMb=150 --vocabulary="conversational-english-16k-lc-16000" --tokenizer=sentencepiece --newOutputFile ;
python3 PreProcessDataFiles.py --inputData="./input-data/synthetic-short-**/**.txt" --outputFileName="synthetic-short-conversations-16k" --isTokenized --forceLowerCase --newOutputFile --outputBatchSizeMb=150 --vocabulary="conversational-english-16k-lc-16000" --tokenizer=sentencepiece --newOutputFile ;
python3 PreProcessDataFiles.py --inputData="./input-data/synthetic-**/**.txt" --outputFileName="synthetic-conversations-16k" --isTokenized --forceLowerCase --newOutputFile --outputBatchSizeMb=500 --vocabulary="conversational-english-16k-lc-16000" --tokenizer=sentencepiece --newOutputFile ;

--train
--8k
python3 TrainModel.py --model="english-conversations-lc-8k-30_0m" --inputData="processed-data/synthetic-casual-conversations-8k/**.bin" --device="mps" --startContext="hey how are you?[eos]" --warmupSteps=300 --evaluationStepFrequency=100 --checkpointStepStorageFrequency=1000 --batchSize=25  --numberOfEpochs=8 --peakLearningRate=0.003 --minimalLearningRate=0.0005 --weightDecay=0.0005 --config="CONV_ENG_LC_8K_CONFIG_XS_512_12L_12H_480E" --newModel 
;
python3 TrainModel.py --model="english-conversations-lc-8k-30_0m-1" --inputData="processed-data/synthetic-short-conversations-8k/**.bin" --device="mps" --startContext="hey how are you?[eos]" --warmupSteps=300 --evaluationStepFrequency=100 --checkpointStepStorageFrequency=1000 --batchSize=25  --numberOfEpochs=8 --peakLearningRate=0.001 --minimalLearningRate=0.0001 --weightDecay=0.0005 --shuffleBatches --config="CONV_ENG_LC_8K_CONFIG_XS_512_12L_12H_480E"  --copyModel

--12k - 19_8m
python3 TrainModel.py --model="english-conversations-lc-12k-19_8m" --inputData="processed-data/synthetic-casual-conversations-12k/**.bin" --device="mps" --startContext="hey how are you?[eos]" --warmupSteps=100 --evaluationStepFrequency=100 --checkpointStepStorageFrequency=1000 --batchSize=25  --numberOfEpochs=4 --peakLearningRate=0.002 --minimalLearningRate=0.0010 --weightDecay=0.0005 --config="CONV_ENG_LC_12K_CONFIG_XS_300_12L_15H_345E" --newModel
;
python3 TrainModel.py --model="english-conversations-lc-12k-19_8m" --inputData="processed-data/synthetic-short-conversations-12k/**.bin" --device="mps" --startContext="hey how are you?[eos]" --warmupSteps=300 --evaluationStepFrequency=100 --checkpointStepStorageFrequency=1000 --batchSize=25  --numberOfEpochs=4 --peakLearningRate=0.0012 --minimalLearningRate=0.0001 --weightDecay=0.0005 --shuffleBatches --config="CONV_ENG_LC_12K_CONFIG_XS_300_12L_15H_345E" --copyModel 

--12k - 31_2m
python3 TrainModel.py --model="english-conversations-lc-12k-31_2m" --inputData="processed-data/synthetic-casual-conversations-12k/**.bin" --device="mps" --startContext="hey how are you?[eos]" --warmupSteps=100 --evaluationStepFrequency=100 --checkpointStepStorageFrequency=1000 --batchSize=25  --numberOfEpochs=4 --peakLearningRate=0.002 --minimalLearningRate=0.0010 --weightDecay=0.0005 --config="CONV_ENG_LC_12K_CONFIG_XS_300_24L_15H_345E" --newModel
;
python3 TrainModel.py --model="english-conversations-lc-12k-31_2m" --inputData="processed-data/synthetic-short-conversations-12k/**.bin" --device="mps" --startContext="hey how are you?[eos]" --warmupSteps=300 --evaluationStepFrequency=100 --checkpointStepStorageFrequency=1000 --batchSize=25  --numberOfEpochs=4 --peakLearningRate=0.0012 --minimalLearningRate=0.0001 --weightDecay=0.0005 --shuffleBatches --config="CONV_ENG_LC_12K_CONFIG_XS_300_24L_15H_345E" --copyModel 

--12k - 39_6m

python3 TrainModel.py --model="english-conversations-lc-12k-39_6m" --inputData="processed-data/english-subtitles-cleaned-000-029-12k/**.bin" --device="mps" --startContext="hey how are you?[eos]" --warmupSteps=100 --evaluationStepFrequency=100 --checkpointStepStorageFrequency=1000 --batchSize=25  --numberOfEpochs=1 --peakLearningRate=0.0005 --minimalLearningRate=0.0001 --weightDecay=0.0005 --config="CONV_ENG_LC_12K_CONFIG_XS_300_6L_15H_690E" --newModel
;
python3 TrainModel.py --model="english-conversations-lc-12k-39_6m-1" --inputData="processed-data/synthetic-casual-conversations-12k/**.bin" --device="mps" --startContext="hey how are you?[eos]" --warmupSteps=100 --evaluationStepFrequency=100 --checkpointStepStorageFrequency=1000 --batchSize=25  --numberOfEpochs=8 --peakLearningRate=0.0005 --minimalLearningRate=0.00001 --weightDecay=0.0005 --config="CONV_ENG_LC_12K_CONFIG_XS_300_6L_15H_690E" --copyModel
;
python3 TrainModel.py --model="english-conversations-lc-12k-39_6m-1-1" --inputData="processed-data/synthetic-short-conversations-12k/**.bin" --device="mps" --startContext="hey how are you?[eos]" --warmupSteps=300 --evaluationStepFrequency=100 --checkpointStepStorageFrequency=1000 --batchSize=25  --numberOfEpochs=12 --peakLearningRate=0.0003 --minimalLearningRate=0.00001 --weightDecay=0.0005 --shuffleBatches --config="CONV_ENG_LC_12K_CONFIG_XS_300_6L_15H_690E" --copyModel 

--12k - 40_2m

python3 TrainModel.py --model="english-conversations-lc-12k-40_2m" --inputData="processed-data/english-subtitles-cleaned-000-029-12k/**.bin" --device="mps" --startContext="hey how are you?[eos]" --warmupSteps=100 --evaluationStepFrequency=100 --checkpointStepStorageFrequency=1000 --batchSize=25  --numberOfEpochs=1 --peakLearningRate=0.0005 --minimalLearningRate=0.0001 --weightDecay=0.0005 --config="CONV_ENG_LC_12K_CONFIG_XS_350_14L_10H_500E" --newModel
;
python3 TrainModel.py --model="english-conversations-lc-12k-40_2m-1" --inputData="processed-data/synthetic-conversations-12k/**.bin" --device="mps" --startContext="hey how are you?[eos]" --warmupSteps=100 --evaluationStepFrequency=100 --checkpointStepStorageFrequency=1000 --batchSize=25  --numberOfEpochs=16 --peakLearningRate=0.0005 --minimalLearningRate=0.000001 --weightDecay=0.0005 --config="CONV_ENG_LC_12K_CONFIG_XS_350_14L_10H_500E" --copyModel



--16k
python3 TrainModel.py --model="english-conversations-lc-16k-26_6m" --inputData="processed-data/synthetic-casual-conversations-16k/**.bin" --device="mps" --startContext="hey how are you?[eos]" --warmupSteps=100 --evaluationStepFrequency=100 --checkpointStepStorageFrequency=1000 --batchSize=25  --numberOfEpochs=6 --peakLearningRate=0.003 --minimalLearningRate=0.0005 --weightDecay=0.0005 --config="CONV_ENG_LC_16K_CONFIG_XS_512_6L_12H_480E" --newModel
;
python3 TrainModel.py --model="english-conversations-lc-16k-26_6m" --inputData="processed-data/synthetic-short-conversations-16k/**.bin" --device="mps" --startContext="hey how are you?[eos]" --warmupSteps=300 --evaluationStepFrequency=100 --checkpointStepStorageFrequency=1000 --batchSize=25  --numberOfEpochs=10 --peakLearningRate=0.0015 --minimalLearningRate=0.0001 --weightDecay=0.0005 --shuffleBatches --config="CONV_ENG_LC_8K_CONFIG_XS_512_6L_12H_480E" --copyModel

