import torch 

CONFIG_NAME = "ConfigName"
VOCABULARY_SIZE = "VocabularySize"

CONTEXT_LENGTH = "ContextLength"
EMBEDDING_DIMENSION = "EmbeddingDimension"
N_HEADS = "NumberOfHeads"
N_LAYERS = "NumberOfLayers"
DROPOUT_EMBEDDING_RATE = "EmbeddingDropoutRate"
DROPOUT_ATTENTION_RATE = "AttentionDropoutRate"
DROPOUT_SHORTCUT_RATE = "TransformerDropoutRate"
QKV_BIAS = "QKVBias"
DEFAULT_DATA_TYPE = "DefaultDataType"

TOKENIZER_TYPE = "TokenizerType"
TOKENIZER_NAME = "TokenizerName"


################################
# CONV_ENG_LC_16K
################################

#__87_604_545
CONV_ENG_LC_16K_CONFIG_XS_500_20L_15H_645E = {
    CONFIG_NAME: "CONV_ENG_LC_16K_CONFIG_XS_500_20L_15H_645E",
    VOCABULARY_SIZE: 16000,
    CONTEXT_LENGTH: 500,
    EMBEDDING_DIMENSION: 645, #45
    N_HEADS: 15,
    N_LAYERS: 20,
    DROPOUT_EMBEDDING_RATE: 0.0,
    DROPOUT_ATTENTION_RATE: 0.0,
    DROPOUT_SHORTCUT_RATE: 0.0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.bfloat16,
    TOKENIZER_TYPE: "sentencepiece",
    TOKENIZER_NAME: "conversational-emotion-english-16k-lc-16000"
}

#__74_276_265
CONV_ENG_LC_16K_CONFIG_XS_500_16L_15H_645E = {
    CONFIG_NAME: "CONV_ENG_LC_16K_CONFIG_XS_500_16L_15H_645E",
    VOCABULARY_SIZE: 16000,
    CONTEXT_LENGTH: 500,
    EMBEDDING_DIMENSION: 645, #45
    N_HEADS: 15,
    N_LAYERS: 16,
    DROPOUT_EMBEDDING_RATE: 0.0,
    DROPOUT_ATTENTION_RATE: 0.0,
    DROPOUT_SHORTCUT_RATE: 0.0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.bfloat16,
    TOKENIZER_TYPE: "sentencepiece",
    TOKENIZER_NAME: "conversational-emotion-english-16k-lc-16000"
}

#33_442_304
CONV_ENG_LC_16K_CONFIG_XS_500_8L_16H_512E = {
    CONFIG_NAME: "CONV_ENG_LC_16K_CONFIG_XS_500_8L_16H_512E",
    VOCABULARY_SIZE: 16000,
    CONTEXT_LENGTH: 500,
    EMBEDDING_DIMENSION: 512, #32
    N_HEADS: 16,
    N_LAYERS: 8,
    DROPOUT_EMBEDDING_RATE: 0.0,
    DROPOUT_ATTENTION_RATE: 0.0,
    DROPOUT_SHORTCUT_RATE: 0.0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.bfloat16,
    TOKENIZER_TYPE: "sentencepiece",
    TOKENIZER_NAME: "conversational-emotion-english-16k-lc-16000"
}


#3_3m
CONV_ENG_LC_16K_CONFIG_XS_200_2L_5H_100E = {
    CONFIG_NAME: "CONV_ENG_LC_16K_CONFIG_XS_200_2L_5H_100E",
    VOCABULARY_SIZE: 16000,
    CONTEXT_LENGTH: 200,
    EMBEDDING_DIMENSION: 100, # 20
    N_HEADS: 5,
    N_LAYERS: 2,
    DROPOUT_EMBEDDING_RATE: 0,
    DROPOUT_ATTENTION_RATE: 0,
    DROPOUT_SHORTCUT_RATE: 0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.bfloat16,
    TOKENIZER_TYPE: "sentencepiece",
    TOKENIZER_NAME: "conversational-emotion-english-16k-lc-16000"
}

################################
# CONV_ENG_LC_12K
################################


# lamda 40_2-1 1
#40_217_500
CONV_ENG_LC_12K_CONFIG_XS_350_14L_10H_500E = {
    CONFIG_NAME: "CONV_ENG_LC_12K_CONFIG_XS_350_14L_10H_500E",
    VOCABULARY_SIZE: 12000,
    CONTEXT_LENGTH: 350,
    EMBEDDING_DIMENSION: 500,
    N_HEADS: 10,
    N_LAYERS: 14,
    DROPOUT_EMBEDDING_RATE: 0,
    DROPOUT_ATTENTION_RATE: 0,
    DROPOUT_SHORTCUT_RATE: 0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.bfloat16,
    TOKENIZER_TYPE: "sentencepiece",
    TOKENIZER_NAME: "conversational-english-12k-lc-12000"
}

#_29_346_304
CONV_ENG_LC_12K_CONFIG_XS_500_8L_16H_512E = {
    CONFIG_NAME: "CONV_ENG_LC_12K_CONFIG_XS_500_8L_16H_512E",
    VOCABULARY_SIZE: 12000,
    CONTEXT_LENGTH: 500,
    EMBEDDING_DIMENSION: 512, #32
    N_HEADS: 16,
    N_LAYERS: 8,
    DROPOUT_EMBEDDING_RATE: 0.0,
    DROPOUT_ATTENTION_RATE: 0.0,
    DROPOUT_SHORTCUT_RATE: 0.0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.bfloat16,
    TOKENIZER_TYPE: "sentencepiece",
    TOKENIZER_NAME: "conversational-emotion-english-12k-lc-12000"
}

#_10_453_248
CONV_ENG_LC_12K_CONFIG_XS_400_8L_8H_256E = {
    CONFIG_NAME: "CONV_ENG_LC_12K_CONFIG_XS_400_8L_8H_256E",
    VOCABULARY_SIZE: 12000,
    CONTEXT_LENGTH: 400,
    EMBEDDING_DIMENSION: 256, #32
    N_HEADS: 8,
    N_LAYERS: 8,
    DROPOUT_EMBEDDING_RATE: 0.0,
    DROPOUT_ATTENTION_RATE: 0.0,
    DROPOUT_SHORTCUT_RATE: 0.0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.bfloat16,
    TOKENIZER_TYPE: "sentencepiece",
    TOKENIZER_NAME: "conversational-emotion-english-12k-lc-12000"
}

################################
# CONV_ENG_LC_8K
################################


#12_735_408
CONV_ENG_LC_8K_CONFIG_XS_350_8L_16H_336E = {
    CONFIG_NAME: "CONV_ENG_LC_8K_CONFIG_XS_350_8L_16H_336E",
    VOCABULARY_SIZE: 8000,
    CONTEXT_LENGTH: 350,
    EMBEDDING_DIMENSION: 336,
    N_HEADS: 16,
    N_LAYERS: 8,
    DROPOUT_EMBEDDING_RATE: 0,
    DROPOUT_ATTENTION_RATE: 0,
    DROPOUT_SHORTCUT_RATE: 0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.bfloat16,
    TOKENIZER_TYPE: "sentencepiece",
    TOKENIZER_NAME: "conversational-emotion-english-8k-lc-8000"
}

#_25_250_304
CONV_ENG_LC_8K_CONFIG_XS_500_8L_16H_512E = {
    CONFIG_NAME: "CONV_ENG_LC_8K_CONFIG_XS_500_8L_16H_512E",
    VOCABULARY_SIZE: 8000,
    CONTEXT_LENGTH: 500,
    EMBEDDING_DIMENSION: 512, #32
    N_HEADS: 16,
    N_LAYERS: 8,
    DROPOUT_EMBEDDING_RATE: 0.0,
    DROPOUT_ATTENTION_RATE: 0.0,
    DROPOUT_SHORTCUT_RATE: 0.0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.bfloat16,
    TOKENIZER_TYPE: "sentencepiece",
    TOKENIZER_NAME: "conversational-emotion-english-8k-lc-8000"
}

#_8_405_248
CONV_ENG_LC_8K_CONFIG_XS_400_8L_8H_256E = {
    CONFIG_NAME: "CONV_ENG_LC_8K_CONFIG_XS_400_8L_8H_256E",
    VOCABULARY_SIZE: 8000,
    CONTEXT_LENGTH: 400,
    EMBEDDING_DIMENSION: 256, #32
    N_HEADS: 8,
    N_LAYERS: 8,
    DROPOUT_EMBEDDING_RATE: 0.0,
    DROPOUT_ATTENTION_RATE: 0.0,
    DROPOUT_SHORTCUT_RATE: 0.0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.bfloat16,
    TOKENIZER_TYPE: "sentencepiece",
    TOKENIZER_NAME: "conversational-emotion-english-8k-lc-8000"
}

################################
# GPT
################################

GPT_CONFIG_SMALL = {
    CONFIG_NAME: "GPT_CONFIG_SMALL",
    VOCABULARY_SIZE: 50257,
    CONTEXT_LENGTH: 1024,
    EMBEDDING_DIMENSION: 768,
    N_HEADS: 12,
    N_LAYERS: 12,
    DROPOUT_EMBEDDING_RATE: 0,
    DROPOUT_ATTENTION_RATE: 0,
    DROPOUT_SHORTCUT_RATE: 0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.float32,
    TOKENIZER_TYPE: "gpt2",
    TOKENIZER_NAME: None
}

GPT_CONFIG_124M = GPT_CONFIG_SMALL
GPT_CONFIG_124M[CONFIG_NAME] = "GPT_CONFIG_124M"

GPT_CONFIG_MEDIUM = {
    CONFIG_NAME: "GPT_CONFIG_MEDIUM",
    VOCABULARY_SIZE: 50257,
    CONTEXT_LENGTH: 1024,
    EMBEDDING_DIMENSION: 1024,
    N_HEADS: 16,
    N_LAYERS: 24,
    DROPOUT_EMBEDDING_RATE: 0,
    DROPOUT_ATTENTION_RATE: 0,
    DROPOUT_SHORTCUT_RATE: 0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.float32,
    TOKENIZER_TYPE: "gpt2",
    TOKENIZER_NAME: None
}

GPT_CONFIG_LARGE = {
    CONFIG_NAME: "GPT_CONFIG_LARGE",
    VOCABULARY_SIZE: 50257,
    CONTEXT_LENGTH: 1024,
    EMBEDDING_DIMENSION: 1280,
    N_HEADS: 20,
    N_LAYERS: 36,
    DROPOUT_EMBEDDING_RATE: 0,
    DROPOUT_ATTENTION_RATE: 0,
    DROPOUT_SHORTCUT_RATE: 0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.float32,
    TOKENIZER_TYPE: "gpt2",
    TOKENIZER_NAME: None
}


GPT_CONFIG_X_LARGE = {
    CONFIG_NAME: "GPT_CONFIG_X_LARGE",
    VOCABULARY_SIZE: 50257,
    CONTEXT_LENGTH: 1024,
    EMBEDDING_DIMENSION: 1600,
    N_HEADS: 25,
    N_LAYERS: 48,
    DROPOUT_EMBEDDING_RATE: 0,
    DROPOUT_ATTENTION_RATE: 0,
    DROPOUT_SHORTCUT_RATE: 0,
    QKV_BIAS: False,
    DEFAULT_DATA_TYPE: torch.float32,
    TOKENIZER_TYPE: "gpt2",
    TOKENIZER_NAME: None
}

modelConfigs = {
    # GPT
    "GPT_CONFIG_SMALL":GPT_CONFIG_SMALL,
    "GPT_CONFIG_124M":GPT_CONFIG_124M,
    "GPT_CONFIG_MEDIUM":GPT_CONFIG_MEDIUM,
    "GPT_CONFIG_LARGE":GPT_CONFIG_LARGE,
    "GPT_CONFIG_X_LARGE":GPT_CONFIG_X_LARGE,

    #CONV_ENG_8K_LC
    "CONV_ENG_LC_8K_CONFIG_XS_400_8L_8H_256E": CONV_ENG_LC_8K_CONFIG_XS_400_8L_8H_256E,
    "CONV_ENG_LC_8K_CONFIG_XS_350_8L_16H_336E": CONV_ENG_LC_8K_CONFIG_XS_350_8L_16H_336E,
    "CONV_ENG_LC_8K_CONFIG_XS_500_8L_16H_512E": CONV_ENG_LC_8K_CONFIG_XS_500_8L_16H_512E,

    #CONV_ENG_12K_LC
    "CONV_ENG_LC_12K_CONFIG_XS_400_8L_8H_256E": CONV_ENG_LC_12K_CONFIG_XS_400_8L_8H_256E,
    "CONV_ENG_LC_12K_CONFIG_XS_350_14L_10H_500E": CONV_ENG_LC_12K_CONFIG_XS_350_14L_10H_500E, 
    "CONV_ENG_LC_12K_CONFIG_XS_500_8L_16H_512E": CONV_ENG_LC_12K_CONFIG_XS_500_8L_16H_512E,

    #CONV_ENG_16K_LC
    "CONV_ENG_LC_16K_CONFIG_XS_200_2L_5H_100E": CONV_ENG_LC_16K_CONFIG_XS_200_2L_5H_100E, 
    "CONV_ENG_LC_16K_CONFIG_XS_500_8L_16H_512E": CONV_ENG_LC_16K_CONFIG_XS_500_8L_16H_512E,
    "CONV_ENG_LC_16K_CONFIG_XS_500_16L_15H_645E": CONV_ENG_LC_16K_CONFIG_XS_500_16L_15H_645E,
    "CONV_ENG_LC_16K_CONFIG_XS_500_20L_15H_645E": CONV_ENG_LC_16K_CONFIG_XS_500_20L_15H_645E
}

def getDataTypeFromConfig(config): 
    # https://medium.com/data-science/pytorch-native-fp8-fedc06f1c9f7
    dt = torch.float32
    cfgDt = config[DEFAULT_DATA_TYPE]
    if cfgDt == 'bfloat16':
        dt = torch.bfloat16
    elif cfgDt == 'float16':
        dt = torch.float16
    elif cfgDt == 'float8_e4m3fn':
        dt = torch.float8_e4m3fn
    elif cfgDt == 'float8_e4m3fnuz':
        dt = torch.float8_e4m3fnuz
    elif cfgDt == 'float8_e5m2':
        dt = torch.float8_e5m2
    elif cfgDt == 'float8_e5m2fnuz':
        dt = torch.float8_e5m2fnuz
    elif cfgDt == 'float8_e8m0fnu':
        dt = torch.float8_e8m0fnu   

    return dt
